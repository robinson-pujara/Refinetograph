# -*- coding: utf-8 -*-
"""CONV_modified.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EuwmPut9bOBoWZQan8P2poLTs-chzFIg
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Add
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing import image
import glob
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import os
import glob
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger
from tensorflow.keras.models import load_model

#  article dependencies
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as Datasets
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tqdm.notebook import tqdm
from tqdm import tqdm as tqdm_regular
import seaborn as sns
from torchvision.utils import make_grid
import random

#  configuring device
if torch.cuda.is_available():
  device = torch.device('cuda:0')
  print('Running on the GPU')
else:
  device = torch.device('cpu')
  print('Running on the CPU')

# Load and preprocess images
img_path = '/content/Sharp'  # Update with your image path
face_images = glob.glob(img_path + '/**/*.jpg', recursive=True)

all_images = []
for i in tqdm(face_images):
    img = image.load_img(i, target_size=(256, 256))
    img = image.img_to_array(img)
    img = img / 255.
    all_images.append(img)







# Convert the list to a numpy array
all_images = np.array(all_images)

# Split data into train and validation data
train_x, val_x = train_test_split(all_images, random_state=32, test_size=0.1)

for img in all_images:
    assert img.shape == (256, 256, 3), f"Found an image with shape {img.shape}"

for img in all_images:
    assert img.shape == (256, 256, 3), f"Found an image with shape {img.shape}"

import numpy as np
import cv2
from tensorflow.keras.layers import Input

def pixelate_image(image, scale_percent=20):  # Decreased scale_percent for more pixelation
    width = int(image.shape[1] * scale_percent / 100)
    height = int(image.shape[0] * scale_percent / 100)
    dim = (width, height)

    small_image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
    low_res_image = cv2.resize(small_image, (256, 256), interpolation=cv2.INTER_AREA)

    return low_res_image

#  train_x and val_x are defined elsewhere in  code
train_x_px = []
for i in range(train_x.shape[0]):
    temp = pixelate_image(train_x[i,:,:,:])
    train_x_px.append(temp)

train_x_px = np.array(train_x_px)

val_x_px = []
for i in range(val_x.shape[0]):
    temp = pixelate_image(val_x[i,:,:,:])
    val_x_px.append(temp)

val_x_px = np.array(val_x_px)

# Sample code to display images - replace this with your actual visualization logic
import matplotlib.pyplot as plt

# Displaying one image from training set
plt.imshow(train_x_px[0])
plt.title('Pixelated Image from Training Set')
plt.show()

# Displaying one image from validation set
plt.imshow(val_x_px[0])
plt.title('Pixelated Image from Validation Set')
plt.show()

# Define the input shape
Input_img = Input(shape=(256, 256, 3))

# Encoder
x1 = Conv2D(256, (3, 3), padding='same')(Input_img)
x1 = BatchNormalization()(x1)
x1 = Activation('relu')(x1)
x2 = Conv2D(128, (3, 3), padding='same')(x1)
x2 = BatchNormalization()(x2)
x2 = Activation('relu')(x2)
x2 = MaxPooling2D((2, 2))(x2)
encoded = Conv2D(64, (3, 3), padding='same')(x2)
encoded = BatchNormalization()(encoded)
encoded = Activation('relu')(encoded)

# Decoder
x3 = Conv2D(64, (3, 3), padding='same')(encoded)
x3 = BatchNormalization()(x3)
x3 = Activation('relu')(x3)
x3 = UpSampling2D((2, 2))(x3)
x4 = Conv2D(128, (3, 3), padding='same')(x3)
x4 = BatchNormalization()(x4)
x4 = Activation('relu')(x4)
x5 = Conv2D(256, (3, 3), padding='same')(x4)
x5 = BatchNormalization()(x5)
x5 = Activation('relu')(x5)

# Adding skip connections
x6 = Add()([x5, x1])
decoded = Conv2D(3, (3, 3), padding='same')(x6)

autoencoder = Model(Input_img, decoded)
autoencoder.compile(optimizer='adam', loss='mse')

import os
import glob
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger
from tensorflow.keras.models import load_model

model_save_path = '/content/Model2'
if not os.path.exists(model_save_path):
    os.makedirs(model_save_path)

checkpoint = ModelCheckpoint(os.path.join(model_save_path, 'model-{epoch:03d}.h5'),
                             monitor='val_loss',
                             verbose=1,
                             save_best_only=False,
                             mode='auto')

early_stopper = EarlyStopping(monitor='val_loss', patience=6, verbose=1)

csv_logger = CSVLogger(os.path.join(model_save_path, 'training_log.csv'), append=True)

# Find the latest model file
list_of_files = glob.glob('/content/Model/*.h5')
latest_model = max(list_of_files, key=os.path.getctime) if list_of_files else None

# Load the latest model if it exists
if latest_model:
    autoencoder = load_model(latest_model)

# Training or resuming training
a_e = autoencoder.fit(train_x_px, train_x,
                      epochs=30,
                      batch_size=1,
                      shuffle=True,
                      validation_data=(val_x_px, val_x),
                      callbacks=[checkpoint, early_stopper, csv_logger])

autoencoder.save('autoencoder_model.h5')  # Save the model

import tensorflow as tf
tf.keras.backend.clear_session()

from tensorflow.keras.models import load_model

# Load the saved model
loaded_model = load_model('/content/Model2/model-009.h5')



from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
import cv2

def load_image(image_path):
    img = image.load_img(image_path, target_size=(256, 256))
    img = image.img_to_array(img)
    img = img / 255.0  # Normalize to [0, 1]
    return img

def pixelate_image(img, scale_percent=10):  # Adjusted scale_percent for more pixelation
    width = int(img.shape[1] * scale_percent / 100)
    height = int(img.shape[0] * scale_percent / 100)
    dim = (width, height)

    small_image = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)
    pixelated_image = cv2.resize(small_image, (256, 256), interpolation=cv2.INTER_AREA)

    return pixelated_image

def denoise_image(model, pixelated_img):
    pixelated_img = np.expand_dims(pixelated_img, axis=0)  # Add batch dimension
    predicted_img = model.predict(pixelated_img)
    return np.squeeze(predicted_img, axis=0)  # Remove batch dimension

# Load and process the clear image
clear_image_path = '/content/viber_image_2022-08-03_10-40-57-674.jpg'
clear_img = load_image(clear_image_path)

# Pixelate the clear image
pixelated_img = pixelate_image(clear_img, scale_percent=20)  # Adjusted scale_percent here

# Use the model to denoise the image
denoised_img = denoise_image(loaded_model, pixelated_img)

# Display the images
plt.figure(figsize=(12, 6))

plt.subplot(1, 3, 2)
plt.imshow(pixelated_img)
plt.title('Pixelated Image')
plt.subplot(1, 3, 3)
plt.imshow(denoised_img)
plt.title('Denoised Image')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load the CSV file
file_path = '/content/convolutional_autoencoder_losses.csv'  # Replace with your file path
losses_df = pd.read_csv(file_path)

# Extracting the necessary columns
epochs = losses_df['epoch']
ssim = losses_df['ssim']
psnr = losses_df['psnr']
d_loss_real = losses_df['d_loss_real']
d_loss_gen = losses_df['d_loss_gen']
d_loss = losses_df['d_loss']
g_loss = losses_df['g_loss']

# Creating a function to plot the graphs
def plot_graph(x, y, title, xlabel, ylabel):
    plt.figure(figsize=(10, 6))
    plt.plot(x, y, marker='o')
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.grid(True)
    plt.show()

# Plotting the graphs
plot_graph(epochs, ssim, 'Structural Similarity Index Over Epochs', 'Epochs', 'SSIM')
plot_graph(epochs, psnr, 'Peak Signal-to-Noise Ratio Over Epochs', 'Epochs', 'PSNR')
plot_graph(epochs, d_loss_real, 'Discriminator Loss for Real Images', 'Epochs', 'Discriminator Loss (Real)')
plot_graph(epochs, d_loss_gen, 'Discriminator Loss for Generated Images', 'Epochs', 'Discriminator Loss (Generated)')
plot_graph(epochs, d_loss, 'Discriminator Loss Over Epochs', 'Epochs', 'Discriminator Loss')
plot_graph(epochs, g_loss, 'Generator Loss Over Epochs', 'Epochs', 'Generator Loss')

